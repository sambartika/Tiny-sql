========================================================================================
How to run the Program

flex typ.l
bison -dy typ.y
g++ -std=c++11 lex.yy.c y.tab.c *.cpp -o a.exe
./a.exe
========================================================================================
Parser:
We have created the parse tree using LEX and YACC. LEX will break the input into tokens and the grammar rules are defined inside YACC. Whatever inputs come, LEX wii break it and YACC 
check if the query is correct. From YACC the parse tree will be generated which will be used in all over the project.

For each non-terminal in the TinySQL grammar, we created a class, with children as pointers to other classes. 
As example, consider the following grammar line for statement:

statement ::= create-table-statement | drop-table-statement | select-statement | delete-statement | insert-statement

Our parser creates a class statement, with children as pointers to create-table-statement class, drop-table-statement class, select-statement class, and so on.

If the input line is a create table statement, then all the pointers in the statement object will be NULL, except the create-table-statement pointer. 

The parser returns a pointer to an object of class statement. Let’s call this pointer as root. We can traverse the entire parse tree structure via this root. pointer 

Once the root of parse tree is generated, the program is routed to db_manager.cpp. 

DB MANAGER

We have defined a db_manager class which has the MainMemory, Disk, SchemaManager objects from the Storage Memory library as its attributes.
Before starting to parse we create an object of db_manager, and initialize it with the storage manager’s Disk, and MainMemory objects. 
 
For each input line that the parser reads, it calls the process_statement function from db_manager, by providing it a root pointer. The process_statement functions evaluates which type of statement the root is of, by evaluating which of its children pointer is NOT NULL: 
INSERT, CREATE, SELECT, DELETE, DROP,

BLOCK MANAGER

In the function process_select_in_memory, the first step we do is to prepare the incoming tables for a join. We have created a function called prepare_join which takes in a vector of table names, and returns just one Relation pointer, which is the joined version of all the tables. 
Now there could only be just one table. In that case it will return the relation pointer of the singleton table, without any join. But if there are multiple tables, then it does the following: 

We do the following optimization, we store the relation size of each relation, and then join them in the order of their sizes, so that the intermediate tables are small, and thus the disk I/O for those tables will be less. To achieve this, we calculate the smallest two relations from the list, and send these two tables to function join, which returns a relation pointer. 

Now while joining two relations we have implemented both one-pass (where one of the join tables fits in memory), the nested loop algorithms (both the join tables are larger than main memory), and sorted two pass (in case of a natural join).

Now joining two tables can result in very large intermediate tables, which needs to be written back to disk, causing more disk I/Os. One optimization that we did was to push the selections down on to the individual join tables if the condition is based solely on the attributes of that column. To achieve this we have written a function which takes the search condition as a postfixExpression, and then creates an expression tree out of it. To this function we pass the attribute list that the current table has, and we try to see if there is a clause in the search condition that can be applied to this table. 

We do that in the following way: we check if the expression begins by an OR, if yes then a single table can’t affect its outcome, and thus we return false, and that tuple has to go for join. 
But if the clause is rooted with an AND, then we check if the column names used in the clause are part of the table attribute names, if yes, then we apply that clause to the tuple, and only proceed the tuple for join if it satisfies the condition.
Once we have checked for the clauses on the individual tables before joining, we join them and then again check if any clause is applicable to it. If yes, we apply that clause, and only write it to the intermediate table if it passes. We write back the intermediate tables back onto disk. And return the pointer of that relation. 

Once we have done the join of all relations, as described above, we delete any intermediate relations created for this join, and only keep the final relation, which is the result of the join, with some selections pushed down on it.

Now we have a relation which is a culmination of the join of the tables. 
Next we implement select in two scenarios, one pass, and two pass; SInce select is just a single block function, we created these two functions because there can be operations in select, like duplicate (DISTINCT), sort (ORDER BY), which can be implemented in either one-pass or two pass based on the relation size compared to the main memory size. We have implemented the one-pass, and two-pass methods of DISTINCT, and ORDER BY. To achieve this we have implemented two pass sort method described in class, and also used the two pass sorting to eliminate duplicates.

In two pass sort, we have used Two-phase Multi-way MergeSort where in the first phase we have first brought into memory maximum number of blocks and sorted them. Then we have added them back to a temporary relation. Then in the second phase we have brought one block from each sorted partition and found the minimum among them and written into a block. When a block is full we have written that to a new relation in disk. When one block of any partition is exhausted, we brought another block from that partition. Like that, we have sorted the relation. Next operations are now performed on new relation.

For two pass distinct, we have first sorted the relation according to the first select list attribute. Then we brought each block from disk. Going backward, if the previous tuple sorting attribute value is different, we have written that. Otherwise while sorting attribute value is same we have checked. If that tuple not found, then written that. Otherwise discarded that tuple.

Select proceeds further by sending each tuple via the search condition check, and it gets printed only if it passes the WHERE clause. 

Once the tuple passes the check, it goes to project function which will select the attributes that needs to be actually printed based on the select list identified earlier.

STORAGE MANAGER LIBRARY
The StorageManager library simulates disk and memory storage in "the memory" of your computer. You will build a single-user Tiny-SQL interpreter upon the StorageManager, storing data in the simulated disk, and get the data from the simulated disk to the simulated memory for further processing. Because the storage is simulated in the memory, everytime your simulated database will start running with empty storage. When the user inputs SQL statements, such as doing insertion, deletion, updating, and querying, the interpreter should access simulated disk and simulated memory. When the database system terminates, the data in the simulated storage are lost.

The data structure of StorageManager is summerized below in a bottom-up fasion:

- Field.h: A field type can be an integer (INT) or a string (STR20).

- Class "Tuple": A tuple equals a record/row in a relation/table. A tuple contains at most MAX_NUM_OF_FIELDS_IN_RELATION=8 fields. Each field in a tuple has offset 0,1,2,... respectively. The order is defined in the schema. You can access a field by its offset or its field name.

- Class "Block": A disk or memory block contains a number of records/tuples that belong to the same relation. A tuple CANNOT be splitted and stored in more than one blocks. Each block is defined to hold as most FIELDS_PER_BLOCK fields. The max number of tuples held in a block can be calculated from the size of a tuple, which is the number of fields in a tuple. 

   The max number of tuples held in a block = FIELDS_PER_BLOCK / num_of_fields_in_tuple

  You can also get the number by calling Schema::getTuplesPerBlock().

- Class "Relation": Each relation is assumed to be stored in consecutive disk blocks on a single track of the disk (That is, in clustered way). The disk blocks on the track are numbered by 0,1,2,... The tuples in a relation cannot be read directly. You have to copy disk blocks of the relation to memory blocks before accessing the tuples inside the blocks. In the other direction, if you need to write to the relation, you have to write to a memory block, and then copy the memory block to a disk block of the relation. The Relation class can be used to create a new Tuple.

- Class "Schema": A schema specifies what a tuple of a partiular relation contains, including field names, and field types in a defined order. The field names and types are given offsets according to the defined order. Every schema specifies at most total MAX_NUM_OF_FIELDS_IN_RELATION=8 fields. The size of a tuple is the total number of fields specified in the schema. The tuple size will affect the number of tuples which can be held in one disk block or memory block.

- Class "SchemaManager": A schema manager stores relations and schemas, and maps a relation name to a relation and the corresponding schema. You will always create a relation through the schema manager by specifying a relation name and a schema. You will also get access to relations from SchemaManager.

- Class "Disk": Simplified assumptions are made for disks. A disk contains many tracks. We assume each relation reside on a single track of blocks on disk. Everytime to read or write blocks of a relation takes time below:

   (AVG_SEEK_TIME + AVG_ROTATION_LATENCY + AVG_TRANSFER_TIME_PER_BLOCK * num_of_consecutive_blocks)

  The number of disk I/Os is calculated by the number of blocks read or written.
  Please NOTE that you do not need to access the Disk directly. Accessing to a Relation is sufficient for any operation. The Relation class will call the Disk automatically.

- Class "MainMemory": The simulated memory holds NUM_OF_BLOCKS_IN_MEMORY blocks numbered by 0,1,2,... When testing the correctness of the interpreter, NUM_OF_BLOCKS_IN_MEMORY will be set to 10. When measuring the performance of the interpreter using one thousand 5-8 field tuples, NUM_OF_BLOCKS_IN_MEMORY will be set to 300. You can get total number of blocks in the memory by calling MainMemory::getMemorySize(). Before accessing data of a relation, you have to copy the disk blocks of a relation to the simulated main memory. Then, access the tuples in the simulated main memory. Or in the other direction, you will copy the memory blocks to disk blocks of a relation when writing data to the relation. Because the size of memory is limited, you have to do the database operations wisely. We assume there is no latency in accessing memory.
